:PROPERTIES:
:ID:       76FF95B7-7784-418B-9B46-5126F6B69BA2
:END:
#+title: Codeklutz
#+author: Shweta Kadam
#+hugo_base_dir: .
#+HUGO_SECTION: posts
#+startup: logdone
#+seq_todo: TODO DRAFT DONE

* Posts
:properties:
:hugo_section: posts
:end:
** DONE Deploying my portfolio website for free on Github Pages using GitHub actions      :githubactions:github:portfolio:freehosting:
CLOSED: [2021-12-05 Mon 23:23]
 :properties:
 :hugo_section: posts/2020/12
 :export_file_name: Deploying my portfolio website for free on Github Pages using Github Actions.md
 :end:
I deployed my [[https://shwetarkadam.github.io/portfolio/][portfolio site]] and wanted to try out github actions and this is my experience of automating the deployment.
This article is more focused on how you can use the GitHub actions and how easy it is to deploy your code to GitHub pages rather than the portfolio site code.So every time you make an update or build to your website ,the changes are automatically reflected and this automated deploying process makes work much faster.

The way GitHub action works is you create actions in your repositories by creating one or more yaml files and these are called workflows.Workflows now can handle build tasks like CI CD. This means you use the action to test your code and push the site to the desired hosting platform (in this case GitHub pages ) when the main branch changes .
First step assuming that you have a GitHub account is to create a repository having your website code in it.Now I have a bootstrap website but in the future I do plan on adding node JS so I already added package.json.

{% gist 7fc9e560ec958d6fb9876019e298e02f %}
#+begin_src json
{
  "name": "shwetarkadam.github.io",
  "version": "1.0.0",
  "description": "Portfolio",
  "main": "index.html",
  "scripts": {
    "build": "npm run clean && npm run imagemin && npm run copyfonts && npm run copydata && npm run usemin",
    "clean": "rimraf dist",
    "copyfonts": "copyfiles -f node_modules/font-awesome/fonts/* dist/fonts",
    "copydata": "copyfiles -f src/js/* dist/js",
    "imagemin": "imagemin src/img/* -o dist/img",
    "lite": "lite-server",
    "start": "npm run lite",
    "test": "echo \"Error: no test specified\" && exit 1",
    "usemin": "usemin index.html -d dist --htmlmin -o dist/index.html"
  },
  "repository": {
    "type": "git",
    "url": "git@github.com:shwetarkadam/portfolio.git"
  },
  "author": "Shweta Kadam",
  "license": "MIT",
  "dependencies": {
    "bootstrap": "^4.4.1",
    "font-awesome": "^4.7.0",
    "jquery": "^3.5.1",
    "popper.js": "^1.16.0"
  },
  "devDependencies": {
    "copyfiles": "^2.2.0",
    "imagemin-cli": "^5.1.0",
    "lite-server": "^2.5.4",
    "rimraf": "^3.0.2",
    "usemin-cli": "^0.6.0"
  }
}
#+end_src

Verify all your changes as correct by first in your root folder running the command:
#+begin_src bash
npm install
#+end_src
npm install

and after installing node modules run the command:
#+begin_src bash
run npm start
#+end_src

so you should get your output in localhost something like this

[[/img/portfolio-githubactions.PNG]]

Now that you have ensured that the project runs properly in your local machine,it is ready to be deployed to GitHub pages. You will only need to commit and push your changes to the main branch of a repo and ensure that the settings are pointing to the correct branch to display a site for that.
Now the file that does this is that deploy.yml file which we will use to create the workflow.

#+begin_src yaml
name: Build and Deploy
on:
  push:
    branches:
      - main
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          persist-credentials: false

      - name: Install and 06Build
        run: |
          npm install
          npm run build
      - name: Deploy
        uses: JamesIves/github-pages-deploy-action@releases/v3
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BRANCH: gh-pages
          FOLDER: dist
#+end_src

Now this yaml file which can be found =.github/workflows/deploy.yml= file in local ,you can rename the file whatever you like.It tells the github actions to install the project dependencies run a build script and put that required files in a output folder name dist and upload the contents of the dist folder to the gh-pages branch and if the branch does not exist it will create that branch.The workflow to deploy the site to github-pages you can find that from James Ives GitHub pages deploy action.
If you have any existing site or code and you want to publish it to get pages you only need this file to be added into your project.
You could go to your github repo Actions Tab -> Create Simple Workflow and copy paste the above content in your yaml file.

Once you have a site ready for GitHub Pages, and your project includes the =.github/workflows/deploy.yml= file, you only need to commit and push your changes to the main branch of your repository. You can the ongoing workflow by going to Actions=>build and deploy.Also this is the place where you can debug what went wrong in case your workflow fails.

[[/img/githubaction-build.PNG]]

[[/img/githubaction-build2.png]]

After the GitHub Actions have run, ensure settings are pointing to the correct branch to display your site.
Go to the settings of your repository and ensure that the source for GitHub Pages is using the correct branch. It is close to the bottom of the main settings page.

[[/img/gh-pages.PNG]]

It does take some time at the start to load in the browser but once available you can click on the link in the green bar above.
Now every time you make a push to the main branch ,the changes are reflected in the main site.

My Portfolio Site: [[https://shwetarkadam.github.io/portfolio/][Click Here]]

That’s all folks.
Happy Learning.
** DONE Polymorphism in Java                                           :polymorphism:java:programming:
CLOSED: [2021-07-14 Mon 23:28]
:properties:
:hugo_section: posts/2021/07
:export_file_name: Polymorphism in Java.md
:end:
Just revisiting and explaining myself Polymorphism concept here through a blog post. The words Polymorphism means multiple forms.

In Java ,Polymorphism means multiple forms of an object. We shall divide this article into 3 sections.

1.Syntax

2.Calling a variable polymorphically.

3.Calling a method polymorphically.

1.SyntaxPermalink
Now in polymorphism in Java, the thumb key rule to remember is

super = subPermalink
Meaning the variable reference (LHS) must always be a super class reference and the object initialization(RHS) must a sub class.

For Example: class A{

} class B extends A{ }
class C extends B{ }
class D extends A{ }

So valid and invalid syntax according to the thumb rule will be
#+begin_src java
A a =new B();           //VALID
B b=new D();            //NOT VALID
C c=new A();           //VALID
A a1=new D();           //VALID

#+end_src
2.Calling a variable polymorphically.Permalink
If a variable is called from a polymorphic object,we follow the reference i.e. the super class. And if the variable is not present in the super class ,it results in a COMPILE ERROR. EG:
#+begin_src java

class A{
int x=5;
}
class B extends A{
int x=10;
}
class App{
public static void main(String[]args){
A a=new B();
System.out.println(a.x);
//What do u think is the output class A x value (5)or class B x value(10)?Follow the rule.

}
}
OUTPUT:
5

#+end_src
Calling a method polymorphically.Permalink
If a method is called from a polymorphic object ,we follow a 2 step procedure: 1.We got to the super class and check whther the method is present or not.
#+begin_src bash
if(present)
 Goto to step 2
else
 COMPILE ERROR

#+end_src

2.Come to the sub class and check wther the method is overrided or not.
#+begin_src bash
if(overrided)
 Call the sub-class version
else
 Call the super -class version.

#+end_src
Eg:
#+begin_src java
class A{
void m1(){
System.out.println("A");
}}
class B extends A{
void m1(){
System.out.println("B");
}}
class App{
public static void main(String[]args){
A a=new B();
a.m1();          //Follow the rule
B=new B();
b.m1();          //Normal sub class object method call
}}
OUTPUT:
B
B
#+end_src
So that’s all for polymorphism in java.

Happy Learning :)
** DONE How constructors work in Java :constructors:java:programming::concepts:
CLOSED: [2021-06-14 Wed 21:41]
:properties:
:hugo_section: posts/2021/06
:export_file_name: How constructors work in Java.md
:end:
Constructors are used every time to initialize instance variables. There are some additional rules associated with constructors that are often asked in interviews.Hence revising those here through a blog post.
************** A constructor is used to initialize instance variables
************** When an object of an class is created,JVM goes to the class and searches for that matching constructor.If Constructor is NOT PRESENT it gives a compile error.
************** By default every class has a constructor called default no argument constructor.
#+begin_src java
class A{
A(){ //default no arg constructor
}}
#+end_src
************** A programmer can have multiple constructors in a class provided their signatures are different this is called constructor overloading.
#+begin_src java
class A{
A(){
//some code
}

A(int x){
//some code
}

A(float x){
//some code
}

A(float x,int y){
//some code
}
A(int x,float y){
}
A(int z){}//THIS WILL GIVE COMPILE ERROR SInce its already defined on top.

}

A a=new A();
new A();//goes to first matching constructor

#+end_src
************** JVM always calls the matching constructor from the class.HOWEVER,a programmer can call other constructors of this class by using the the this() method.
#+begin_src java
class A{
A(){
System.out.println("A");    //I
A(int x){
this();                     //this will go to constructor A();
System.out.println("AA");   //II
}
}
class App{
public static void main(String[]args){
new A(5);
}}

OUTPUT:
A
AA


#+end_src
************** If a programmer desires it can call the constructor of the super class as well from its own constructor using the super() method.
#+begin_src java
class A{
A(){
System.out.println("A");    //I
}
}
class B extends A{
B(){
super();             //this is called implicitly refer next point also
System.out.println("B");
}}


class HelloWorld {
    public static void main(String[] args) {
        new B();
    }
}
OUTPUT:
A
B
#+end_src
************** Whenever a programmer creates a constructor ,JVM writes super() in every constructor implicitly as its first line.
Note:If a class does not extend any class it by default extends the Object class. Do Try this code in your ide to see it for yourself
#+begin_src java
class A{
A(){
//super will be called implicitly at the first line of this constructor and here since it does not extend any class it will extend the Object class
System.out.println("A");    //I
}

A(int x){
//super will be called implicitly at the first line of this constructor
System.out.println("AA");
}}


class HelloWorld {
    public static void main(String[] args) {
        new A(5);
    }
}
OUTPUT:
A
AA
#+end_src
That’s all for constructors in Java.

Happy Learning :)
** DONE Integrating Swagger OpenAPI for easy API documentation in spring boot :api:apidocumentation:restapi:springboot:swaggeropenapi:
CLOSED: [2022-01-16 Wed 22:53]
:properties:
:hugo_section: posts/2022/01
:export_file_name: Integrating Swagger OpenAPI for easy API documentation in spring boot.md
:end:
These days I am more into creating backend projects which include microservices.But if anyone wants to test these services one needs postman or do the old classic way of curl command.

Both do the job brilliantly but what if I wanted some user who doesn’t want to install postman or use curl and still wants to test my live APIs thru the browser? I came across this **swagger open API specification**  and this is a really handy tool!

In layman’s terms, Swagger OpenAPI specification provides API documentation for REST APIs. An OpenAPI file allows you to describe all the APIs within the project and even lets you try out the APIs!

Available endpoints can be /projectApi and all operations on each endpoint which can GET /getProjectApi , POST /insertProjectApi , DELETE /deleteProjectApi .

[[/img/swaggerui.PNG]]
Also, integration of swagger open API is pretty painless in spring boot and it lets users try out the APIs within the browser without any installation of any software from the user (sounds pretty convenient and sweet to me)

In this post, I will describe how I integrated swagger open API in Spring boot project.First you need a spring boot project having basic dependcies using Spring Initializr https://start.spring.io/ or you could use this project used in the example here

First add the springdoc-openapi-ui dependency to pom.xml:
#+begin_src xml
<dependency>
   <groupId>org.springdoc</groupId>
   <artifactId>springdoc-openapi-ui</artifactId>
   <version>1.6.4</version>
</dependency>
<dependency>
#+end_src
Then run the application and check the below url to check open api specification
#+begin_src bash
http://localhost:8080/v3/api-docs/
#+end_src
You should be able to see something like this
[[/img/open-apidocs.PNG]]
You can also add a custom path by adding entry in **application.properties** file
 #+begin_src bash
springdoc.api-docs.path=/api
springdoc.swagger-ui.path=/swagger
springdoc.swagger-ui.operationsSorter=method
 #+end_src
 [[/img/custom-open-apidocs.PNG]]
 Check http://localhost:8080/swagger for web UI.To show you in this example we have a following apis in the controller
 #+begin_src java
package com.TestDocker.BooksDocker.Controllers;

import java.util.List;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.server.ResponseStatusException;

import com.TestDocker.BooksDocker.Models.Book;
import com.TestDocker.BooksDocker.Repository.BookRepository;

@RestController
public class MainController {
	@Autowired
	public BookRepository bookRepository;

	@GetMapping("/test")
	public String test() {
		return new String("Working from DOcker Bopoks proj ");
	}
	@GetMapping("/")
	public List<Book> fetchAllBooks() {
		List<Book> books;
		try {
			books = bookRepository.findAll();

		} catch (Exception ex) {
			throw new ResponseStatusException(HttpStatus.INTERNAL_SERVER_ERROR, "Error occured in fetchAllBooks", ex);

		}
		return books;
	}

	@GetMapping("/{bookID}")
	public Book fetchBookfromID(@PathVariable("bookID") Long bookID) {
		Book book;
		try {
			book = bookRepository.getById(bookID);

		} catch (Exception ex) {
			throw new ResponseStatusException(HttpStatus.INTERNAL_SERVER_ERROR, "Error Occured in fetchBookfromID", ex);

		}
		return book;
	}

	@GetMapping("/search/{title}")
	public List<Book> searchBookByTitle(@PathVariable("title") String title) {
		List<Book> books;
		try {
			//System.out.println(title);
			books = bookRepository.fuzzySearchByTitle(title);
			System.out.println(books);

		} catch (Exception ex) {
			throw new ResponseStatusException(HttpStatus.INTERNAL_SERVER_ERROR, "Error Occured in searchBookByTitle", ex);
		}
		return books;
	}

	@PostMapping("/insertBooks")
	public String insertBooks(@RequestBody List<Book> books) {

		for (Book b : books) {
			System.out.println(b.toString());
			Book b1 = bookRepository.save(b);
			if (b1 == null)
				return "Book object is null";
		}
		return null;
	}
}
 #+end_src
 So the swagger ui look something like this.
 Also json docs will be available at http://localhost:8080/api springdoc.swagger-ui.operationsSorter=method sorts the API paths in order of their HTTP methods.
You can try and test the apis from web ui too.It also shows schema information!
Overall this is a much convenient way of setting up documentation for your apis which can be handy in some situations.

That’s all folks!
** DONE Making Peace with Windows!Installing wsl,zsh,powerlevel10k,fzf & many more fun plugins for easy development :zsh:wsl:productivity:git:ohmyzsh:config:fzf:powerlevel10k:
CLOSED: [2022-01-21 Wed 23:41]
:properties:
:hugo_section: posts/2022/01
:export_file_name: Making Peace with Windows!Installing wsl,zsh,powerlevel10k,fzf & many more fun plugins for easy development.md
:end:
avigation, editing, development using terminal and zsh . But recently due to unforeseen updates, my bios was messed up big time which has led me unable to install Linux for the time being. But the work and learning never stops and nor shall I ! ☺  I don’t hate windows but it’s definitely not my first choice for development and coding after discovering Linux.🤭

But Thanks to WSL, windows terminal, and the beautiful zsh .I can get that Linux experience on windows!

So this is just a blog post on how I customized my terminal on windows 10 using wsl, windows terminal,zsh, and many more fun plugins which I use on my Linux as well as windows for development(work or home).
[[/img/zsh.PNG]]
*** What is wsl?
It stands for windows subsystem for Linux and it's a feature of Windows that allows developers to run Linux file systems,command-line tools etc directly on windows!(Goodbye painful windows mouse navigation) First, you need the wsl feature on windows 10 by going to =Start ->Type windows feature on search and below checkbox should be checked for enabling windows subsystem for Linux.=
[[/img/windowsfeaturecheck.PNG]]
Now you need to install wsl which you can by going =Start-> Microsoft store ->type ubuntu=.Im installing Ubuntu wsl since I'm familiar with it you can also change distros. I'm also installing another app called windows terminal because it's much better in terms of ui to me as compared to Ubuntu terminal.This is optional.
[[/img/terminalcomparision.PNG]]
At this point, it's your choice whether you want to continue with the Ubuntu terminal or use the windows terminal.If you decide with the former,skip the next para and if you decide with the latter then you need to set windows terminal as your default shell.
[[/img/windowstermsettings.PNG]]

Now by default windows terminal opens the power shell, to set to Ubuntu
[[/img/setdefaultshell.PNG]]
Go to settings as shown below

Now you have a Ubuntu shell that has bash. I personally use zsh with OhMyZsh for my work for that beautiful productivity. Using OhMyZsh features like navigating without using cd, usage of ll, easier tab-click based navigation, and much more!

Note that zsh and OhMyZsh are different. When you install OhMyZsh, many plugins come with it for your rescue! So to install zsh. Update the libraries first then install zsh.
#+begin_src bash
sudo apt-get install update
sudo apt install -y zsh
#+end_src
Then Install ohmy zsh
#+begin_src bash
sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
#+end_src
Now your previous~/.zshrc config will be replaced by ohmyzsh To customize the shell next install powerlevel10k.
#+begin_src bash
git clone - depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k
#+end_src
This command clones the repo and now go to your ~/.zshrc and set the theme as power level 10k And then
#+begin_src bash
source ~/ .zshrc 
#+end_src
Note: To reflect every change you make, do =source ~/.zshrc= in the terminal. And then this will give you a set of options to configure which you can decide for your customization.
**** My favourite plugins
I use these plugins daily and they make my life super smooth !
***** Fzf
It's a fuzzy finder command-line tool that lets your fuzzy find anything (files directories git branches you name it )across file system. You can use ti write your custom fuzzy find scripts to find anything.I have posted a link if my current config and aliases for reference. Clone the repo from any directory and just run the install script.
#+begin_src bash
git clone - depth 1 https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install
#+end_src
[[/img/fzf.PNG]]
Here is a small example of small WIP config for reference.
***** Zsh Auto-suggestions
This one Autocompletes while you type a command.This is useful especially when you type commands which you use daily but need to to try multiple times such as navigating and printing log at a specific long location. Git Clone the zsh-autocomplete plugin in the OhMyZsh plugin folder.
#+begin_src bash
$ sudo git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
#+end_src
Once that is done, add the plugin in the ~/.zshrc file's plugin list.
#+begin_src bash
plugins=(
 …
 zsh-autosuggestions
)
#+end_src
[[/img/zshautosuggest.PNG]]
***** Zsh Syntax highlighting
This one automatically highlights zsh commands as you type. This saves a lot of typing on my part. Git Clone the zsh-syntax-highlighting plugin in the OhMyZsh plugin folder.
#+begin_src bash
$ sudo git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting
#+end_src
And once again add it to the plugins list of the .zshrc file.
#+begin_src bash
plugins=(
 …
 zsh-syntax-highlighting
)
#+end_src
Note: To reflect every change you make, do source ~/.zshrc in the terminal.
***** Readymade Github Aliases from Oh My Zsh
Usually one defines short github aliases such as g.b forgit branch or g.c for git checkout in ~/.zshrc but you know by using ohMyZsh already has a list of easy git aliases configured. The format is first 2–3 letters of the first letter of the command such as
#+begin_src bash
gb git branch List of local branches
gba git branch -a List of local and remote branches
gcam git commit -am Add all files to stage and commit
gcmsg git commit -m Git commit message
gco git checkout Change branch
gco - git checkout to the previous branch Change branch to the previous one
gd git diff Files differences in staging
gfa git fetch - all - prune Fetch all remote branches, delete branch if upstream is gone
gl git pull Pull from remote
gp git push Push to remote
gpsup git push - set-upstream origin [currentbranch] Set upstream branch
gst git status Local files to commit
#+end_src
I use these git aliases daily and they make working super fun.I recommend going through oh-my-zsh git aliases cheatsheets

That's all folks! This is my current setup in windows for development and this is still a work in progress that can keep changing but these plugins and zsh are something that has made the experience of using windows quite fun.
** DONE Which would you go for? Spring boot cron job,scheduled tasks vs Events in Mysql. :cronjob:debugging:development:events:java:mysql:spring:springboot:sql:
CLOSED: [2022-01-12 Wed 23:49]
:properties:
:hugo_section: posts/2022/01
:export_file_name: Which would you go for? Spring boot cron job,scheduled tasks vs Events in Mysql.md
:end:
I was recently studying about using cron jobs in spring boot for a particular use case for my small side project. I ended up not using the cron job but rather went the SQL way(will explain this in detail below). However,in the process I learnt a lot about cron jobs and scheduling in spring boot so this is just a small article about my learnings.

But first I shall tell you a little about my use case and why I thought about cron jobs in the first place…..
*** Use case
My application was inserting data (let’s call it smash data for simplicity for now)in the database.Each smash data has a certain expiry period and after that expiry period, that data should no longer remain in the database.But the expiry period will be different for each smash data.

Example:

smash 1, expiryperiod :10mins

smash 2 ,expiryperiod :60mins

smash 3 ,expiryperiod :150mins . . . etc.

Now my first line of thinking ended up being cron jobs which led to me studying about cron jobs and scheduled in spring boot.To answer it simply I didn’t end up taking this route is because cron jobs or scheduled tasks are suited when we expect the task to execute at only a particular point of time or where we expect functionality to be executed at w particular time on an hourly/daily /weekly/monthly basis. I could get the cron job to delete the data but to delete WHICH data smash 1 or smash 2? That would mean I would have to check the DB. So the process would be something like:-

Fetch all rows from DB.
Check timestamp of each row data against current timestamp and delete accordingly.
I wanted to avoid writing the searching, comparing time logic (dates, in general, can be a pain sometimes).The logic which I did ended up going through was events in SQL since I’m using MySQL db for the use case
#+begin_quote
Mysql events are tasks that run according to a particular schedule …hence they can be called as scheduled events
#+end_quote
When an event is created in MySQL, a named database object is created and this object consists of one or more SQL statements to be executed at some regular intervals.Using events,I didn’t have to retrieve and search the data (as I had to do in the spring boot controller ) . I could just write an event such as
#+begin_src sql
Delete from table1 where
expiry period < NOW();
#+end_src
And schedule this to execute =every minute=. Which was would check for that expiryPeriod column in each row and compare with time NOW() So any rows whose expiryperiod has passed will be deleted from db.

The only thing to note I see in this approach, for now, is that this is database dependent so when I host this side project (a hopeful dream) I need to make sure events is configured for the same. So this was the use case now back to cron jobs!
*** Cron jobs or schedule tasks in spring boot.Permalink
When a situation arises where we expect the task to execute at only a particular point of time or where we expect functionality to be executed at a particular time on an hourly/daily /weekly/monthly basis. Cron jobs are suitable for this use case. In spring this sort of scheduled task can be achieved through @Scheduled annotation.

There are a few rules while using the @Scheduled annotation:  1. The method should typically have a void return type else the returned value will be ignored.

the method should not expect any parameters. First, to enable scheduling in the spring boot project, use @EnableScheduling in the main class.
#+begin_src java
public class Application {
 public static void main(String[] args) {
 SpringApplication.run(PasteBinApplication.class, args);
 }
}

#+end_src
*** Scheduling using CRON expressions
#+begin_src java
@Component
public class SchedulerService {
    @Scheduled(cron="*/15 * * * * ?")
    public void testScheduled()
    {
        System.out.println("Method executed at every 15 seconds. Current time is :: "+ new Date());
    }
}

#+end_src
 A guide for cron jobs: cron Image source :Java Techonline
 [[/img/cron.PNG]]
 #+begin_src bash
SEC  MIN   HOURS   DAY  MONTH  WEEKDAY
 *    *      *      *     *      *

 #+end_src
*** Scheduling using initial delay,Fixed Delay or Fixed Rate
 The main difference between Fixed Delay and Fixed Rate is : Fixed Delay : controls the next execution time when the last execution finishes. Fixed Rate : makes Spring run the task on periodic intervals even if the last invocation may be still running.

Fixed Delay
#+begin_src java
@Component
public class SchedulerService {
    @Scheduled(fixedDelay = 1000, initialDelay = 5000)
    public void testScheduled()
    {
        System.out.println("Method executed with fixed delay and initial delay . Current time is :: "+ new Date());
    }
}
#+end_src
 Also Fixed Delay can take input in String and Integer. @Scheduled(fixedDelayString = “7000”) @Scheduled(fixedDelayString = 7000)
 Fixed Rate:
 #+begin_src java
@Component
public class SchedulerService {
    @Scheduled(fixedRate = 1000)
    public void testScheduled()
    {
        System.out.println("Method executed with fixed rate . Current time is :: "+ new Date());
    }
}
 #+end_src
That’s all folks.Learning about events and cron jobs and where could be applied was interesting to learn when applied on some small practical application.
** DONE Tech Recap Journal- January📓 :blog:debugging:domain:retrospect:lessions:note:experiment:
CLOSED: [2022-02-01 Thu 15:15]
:properties:
:hugo_section: posts/2022/02
:export_file_name: Tech Recap Journal- January.md
 :end:
 I tried a lot of things in January not necessarily everything learnt was used and and not every side project which I worked on got live.

However I learnt many lessons from my own failures and gained more insights when I started some initiatives. So just a small gist of looking back on January and mid February.
*** My Blog! [[https://codeklutz.com][codeklutz.com]]
I have been wanting to make my own tech blog for a while now but I needed something which didn’t necessary requires much code or db maintenance.I didn’t want to opt for WordPress for the same reason.

Jekyll with GitHub pages is a life saver here! Also learnt a lot on custom domains after buying my own domain,about Google analytic and SEO.Plus customising Jekyll site with themes has been fun.
*** Letters to me
This idea struck me in the wee hours at night.I always get some random tech ideas or where I am curious about something and think about it as to how I would do this particular task.

I think of this site as an idea jar 💡 or tech journal 📝 where I jot down my wacky, scrambling thoughts.Something which I can look back on for ideas when I don’t feel creative or as starting thinking point for my small side projects.Some tech thoughts which aren’t polished enough for a blog but are useful tiny ideas which provide insight.
Also since it’s on the internet maybe someone might find it useful or insightful? I’m thinking of adding an rss feed to this in the future if anyone would be interested in following this.

*** Expiermenting with audio in blogs
Based on the idea mentioned in letters.codeklutz.com decided to implement an audio feature for this blog.I tried finding some open source or free alternative.And I did find one but sadly this one proved to be a failure at the current moment.

The audio is decent but the voices which I found are too mechanical and monotonous to listen continuously.I shall still try finding some open source alternative because I don’t want to invest in paid alternatives for this small blog at the moment.
*** NoteKlutz [[https://note.codeklutz.com]]
This mini project is again a part of implementation of the idea mentioned in letters.codeklutz.com It already proving to be useful;) I realised I write more markdown (for creating study notes +writing this blog) so I felt the need to create my own editor which is suited for myself and at the same time not fear giving my data to someone or idk?(lmao)

But main focus was not to use another app just to create notes and since I use browser more than anything else this seemed like a good idea

It’s a small,minimal project which does exactly what I need it to do and does it right (at least for me biased here🤫) It’s like my second organising brain 🧠.The code is an absolute mess and needs heavy work which I will do my pushing small updates on weekends🤭.

So this was all for January and Mid Feb
** DONE Tackling procrastination and kubernetes study :kubernetes:basics:blog:notes:learning:kubeapiserver:
CLOSED: [2022-02-25 Thu 15:45]
:properties:
:hugo_section: posts/2022/02
:export_file_name: Tackling procrastination and kubernetes study.md
:end:
After a long series of procrastination and getting the hit of motivation from reading Atomic Habits(great book which I recommend others) ,I’m finally learning kubernetes basics.As a motivator to get better at writing and publish more posts as well as learn kubernetes.I have decided to publish 1 article every Sunday.I would like to post 2 posts per week but I want to start small and consistent. Once again I’m treating my blog as a journal to showcase how much I actually understand kubernetes.Also its quite handy to have my own notes on a site. So here is a blog post on kubernetes basics part 1.This will be a multi part series. Before we begin some pre requisites which one needs to know :-
******** Pre requisite
You should be already familiar and comfortable with the concept of containers and container run-time such as docker as kubernetes is for managing different containers and their deployment at a large scale.Another point which is not mandatory but good to know would be basic docker commands like docker run etc.
******** What is kubernetes?
Kubernetes is an open-source technology that is used for container orchestration. And what is container orchestration exactly? It is the process of continuous deployment ,scaling and management of containers.

Lets first look at the kubernetes architecture and the individual components in it.
******** Node:
A Node is either a physical or virtual machine on wihc kubernetes is installed. A node is like a worker machine on which containers (having our application) will be running by Kubernetes.And like any other machine ,nodes can crash for a number of reasons ;) .So once the node crashes the application will be go down as well. So tackle this we need multiple nodes rather than 1 node.

And mulitple nodes come together to form a group known as the cluster.So even if one node inside the cluster fails,we have our application accessible and running from the other nodes.Plus it helps in sharing load!
******** * Master Node :

So now we have our cluster running on a group of nodes which are running our containerised apps.But who is responsible to manage this cluster:?Also when a node goes down how to direct the traffic of the failed node to other working nodes?Also who stores the information about these worker nodes stored? and How are the nodes monitored?

The master node!

The master node is another machine with kubernetes installed in it and it watches over the nodes and does the actusl orchestration of the worker nodes.
#+begin_quote
Note that a cluster can have multiple master nodes depending on the size of the cluster.


#+end_quote
because at the end of the day , a master node is a machine (which can crash) and for high availability we need to avoid that.
******** Other components:
When you install kubernetes in your system,you are actually installing the follwing components:

- An Kube api server (Master)
- An etcd service (Master)
-A kubelet service (Worker)
- Controller (Master)
- Scheduler (Master)
- Container Runtime
******** Kube API server:
Kubeapi server acts as a frontend for kubernetes.The users,commandline tools,managment devices all interact with the cluster via the Kube API server.
******** etcd
etcd is a distributed key value store used to store data about how to manage the cluster.It is also resp0onsible to implement logs within the cluster to ensure there is no conflict between mulitple masters.

#+begin_quote
Note that your application data is not stored in etcd only logs and information about the cluster.
#+end_quote
******** Scheduler
Scheduler is responsible is distributing containers across multiple nodes.It looks for newly created containers and assigns them to nodes.
******** Controllers
Controllers are the brain behind the orchestration. They are responsible for notcining and responding nodes,containers or endpoint goes down.The controller takes decsions to bring up new nodes in this case.

******** Container Runtime
The container runtime is the underying software that runs the containers.Most of the times,its docker.I have used docker but there are other runtimes such as CRI-O

******** KubeletPermalink
Kubelet is an agent that runs on each worker node in the cluster. The agent is in charge of making sure that containers are running on the nodes as expected.

******** Master vs Worker nodesPermalink
So now we know there are 2 types of nodes : Masternode and Worker node. How does a node become master or a worker node? A worker node has the containers are hosted and running .Hence to run these containers we need a Container Runtime such as docker installed in these machines.

The master has a kube API server and this is what differentiates the master from worker nodes. The worker node have the kubelet agent that interacts with the master to proivide health information about the worker nodes and carries out the instrcutions given by the master node on worker nodes.The master has all this information stored in key value store (etcd) known as the etcd.The master also has the controller and scheduler.

********   KubectlPermalink
kubectl is a commandline tool is used to deploy and manage applications in a cluster.Basically we are going to use these commands from the kubectl tool to get us information (kubectl get,status describe) about the nodes and other components in the cluster and to manage many other operations.

=kubectl run= –Used to deploy an application onto the cluster

=kubectl get cluster-info= –Used to fetch the cluster information

=kubectl get nodes= –Used to fetch information about nodes.

That is all on basic overview.Next article will be focused on pod and how pods work in nodes in kubernetes.
** DONE Today I learnt: Database Version Control :til:flyway:liquibase:databaseversioncontrol:databaseschemachanges:
CLOSED: [2022-11-07 Thu 15:49]
:properties:
:hugo_section: posts/2022/11
:export_file_name: Today I learnt: Database Version Control.md
:end:
Upon stumbling upon this motivating HN post by [[https://simonwillison.net/2022/Nov/6/what-to-blog-about/][Simon Willison]] I have been inspired to start a Today I learnt(TIL) series of my own. This seems like a doable promising idea where I do not have the self-imposed pressure of researching for a blog idea and making a seperate time to write that specific post. Wrting this TIL flows naturally in day-to-day work flow where I could just say “Hey I just learnt about this XYZ ,I should write about it”.

Starting with Today’s TIL : **Database Version Control**

What it is : A practice or form of maintining and tracking every change made to database schema, just like git version control(But this is specifically for Database). It acts like a single source of truth (like a git code repository)

This concept solves a lot of problems we face as developers such as :

As a developer,One must have faced a situation where to solve a particular problem statement or feature , you need to do database changes,however for those changes to reflect application needs to be restarted or you might have database and application code changes, an organization already has some processes defined for deployment. In development phase, one usually runs the db changes or sql queries in local generally via a sql client application.For example,Update some existing db property.

But for that same change to be reflected in production db, CICD processes are define fdd for deployment or a seperate team might be responsible for deployment altogether.Hence we cant expect a seperate db team to always be in sync with deployment team or that particular CICD process. Hence, DB schema changes should be deployed as a part of application code changes.

This is where database version control comes in handy where :

You need traceability and a commit history of db schema changes done before.
Protect prodcution database tables from unwanted or uncontrolled changes
Help in communication between teams regarding data(where a member can look at the query and provide feedback as a part of Pull request)
Applications such as liquidbase,flyway scripts come in handy Speaking of liquibase which works on changelog concept where you have

#+begin_quote
A Changelog file which inside has -> ChangeSet (which are used to define Db changes)- > Which can include SQL Queries and Rollback queries if the changes dont work in that specific environment.
#+end_quote
#+begin_src xml
<?xml version="1.0" encoding="UTF-8"?>
<databaseChangeLog
	xmlns="http://www.liquibase.org/xml/ns/dbchangelog"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:ext="http://www.liquibase.org/xml/ns/dbchangelog-ext"
	xmlns:pro="http://www.liquibase.org/xml/ns/pro"
	xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog
		http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-latest.xsd
		http://www.liquibase.org/xml/ns/dbchangelog-ext http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-ext.xsd
		http://www.liquibase.org/xml/ns/pro http://www.liquibase.org/xml/ns/pro/liquibase-pro-latest.xsd">


     <changeSet  id="1"  author="XYZ">
        <comment>

        </comment>
        <sql>
        INSERT INTO Exampledb.exampletable('id','name,'serial')
        VALUES("1","test","serial");
        </sql>

        <rollback>
        DELETE FROM  Exampledb.exampletable where id="1";
        </rollback>
 </databaseChangeLog>


#+end_src
This changelog file needs to be included in a master changelog file which consists a list of all change log files (Similar to how a git commit history consists of all commit ids)
** DONE Today I learnt: Interesting Things in Java 11 :arrays:blog:primitives:var:java11:
CLOSED: [2022-11-20 Thu 18:48]
:properties:
:hugo_section: posts/2022/11
:export_file_name: Today I learnt: Interesting Things in Java 11.md
:end:
I came across a fascinating Java talk on youtube by Devoxx 2022 Hanno Embregts. This article is about a few java snippets I encountered. The purpose of today’s TIL is to have a list of interesting things we could do in Java and not deep dive into each concept.

Today’s TIL : **Crazy things to do with Java 11+**

**** Initializing Array and var keywordPermalink
Having the =var= keyword in a statically typed language such as Java was fascinating in itself(an article on this in the future :). But we never thought we would use it to initialize such as
#+begin_src java
var element =new int[2];       //WORKS
var [] element=new int[2];      // COMPILE ERROR :error: 'var' is not allowed as an element type of an array
Since var is a generic element type, giving it array [] provides an error since rather than being generic we are giving it an array type.

#+end_src

**** C style ArrayPermalink
What is a c style array? Java supports providing [] before and after the variable name in an array
#+begin_src java
 int []arr=new int[2];
 int arr1[]=new int[2];

#+end_src
In C style array, we provide [] after the variable name that is ~int arr1[].

So in Java, suppose we have the following code:
#+begin_src java
       int arr1[],arr2;
       arr1=new int[1];
       arr2=new int[1];        //COMPILE ERROR : error: incompatible types: int[] cannot be converted to int

#+end_src
The above code will result in COMPILE ERROR for arr2 since arr2 is not an array but a primitive int variable. But if we want to want both arr1 and arr12 as array type we need to change the declaration to
#+begin_src java
       int [] arr1,arr2; //Notice how [] are
       arr1=new int[1];
       arr2=new int[1];

#+end_src
**** Arrays.asList and Primitives
Let’s look at the following example :
#+begin_src java
String [] strArr={"one","two","three"};
var stringList= Arrays.asList(strArr);

int [] intArray = {1,2,3};
var intList = Arrays.asList(intArray);

System.out.println(stringList.contains("one")+" ");
System.out.print(intList.contains(1));

Output: true false

#+end_src

Signature of Arrays.asList is var-args or List of T’s.
#+begin_src java
      public static <T> List<T> asList(T... a)

#+end_src

T is of generic type so that means it needs to reference a Type such as Integer,Float and not reference Array of Ints

But next question is Can they boxed ? (Autoboxing: Converting primitive to Class Type example : int -> Integer) Answer is no Array of ints -> Cannot be boxed -> Array of Integer
#+begin_quote
Don’t use Arrays.asList on primitives

#+end_quote

**** No structural changes allowed in ArrayPermalink
#+begin_src java
String [] ints ={"a","b","c",null};
List<String> strings= Arrays.asList(ints);
strings.removeIf(Objects :: isNull);
System.out.println(strings.size());
Output: Exception in thread “main” java.lang.UnsupportedOperationException: remove at java.base/java.util.Iterator.remove(Iterator.java:102)

#+end_src

Because the array does not allow any structural changes to it

**** A Unique way to remove null values from Map
#+begin_src java
Map<Integer,String> map=new HashMap();
map.put(4,null); //currently map has key:4 value: null
System.out.println(map.getOrDefault(4,"four"));
map.putIfAbsent(4,"four");     //key key:4 value:four
System.out.println(map.get(4));

Output: null,four


#+end_src
##
#+begin_src java
var numbers = List.of(-1,0,1);
Map<Integer,List<Integer>> map=new HashMap<>();
numbers.forEach(number-> map.putIfAbsent(number,new ArrayList<>())
               .add(number));
System.out.println(map.get(0));

Output: NullPointerException: Exception in thread “main” java.lang.NullPointerException at HelloWorld.lambda$main$0(HelloWorld.java:33)

#+end_src

Because map.putIfAbsent returns null if no value is present
** DONE Today I learnt:422 HTTP Error code :http:422:422unprocessableentity:httperrorcode:
CLOSED: [2022-11-20 Thu 19:38]
:properties:
:hugo_section: posts/2022/11
:export_file_name: Today I learnt:422 HTTP Error code.md
:end:
Today while testing a soap API at work, I came across this HTTP error code called **HTTP/1.1 422 Unprocessable Entity** . According to MDN Web docs, it means the following :
#+begin_quote

The HyperText Transfer Protocol (HTTP) 422 Unprocessable Entity response status code indicates that the server understands the content type of the request entity, and the syntax of the request entity is correct, but it was unable to process the contained instructions.

#+end_quote

It means that the syntax of the request is correct and well-formed but it has semantic, logical errors. Meaning the soap body xml format will be correct but the issue arises because of the content inside the XML tags*. In this case, it could be the following:

- Wrong character within the code.
- The server doesn’t understand the content within a particular XML tag.
- Or it refuses to process any other content inside the tag other than a fixed decided value).
Difference between 422,404 and 415 Error codes .Permalink
According to [[https://www.rfc-editor.org/rfc/rfc4918#section-11.2][RFC]],
#+begin_quote

The 422 (Unprocessable Entity) status code means the server understands the content type of the request entity (hence a 415(Unsupported Media Type) status code is inappropriate), and the syntax of the request entity is correct (thus a 400 (Bad Request) status code is inappropriate) but was unable to process the contained instructions. For example, this error condition may occur if an XML request body contains well-formed (i.e., syntactically correct), but semantically erroneous, XML instructions.


#+end_quote
In testing a soap API, the content type of the request body was correct. I checked for Content-Type header while testing) – Hence 415(Unsupported Media Type) was not valid. For soap,it was Content-Type: text/xml; charset=UTF-8. And the syntax of the request body was also correct– Hence 404(Bad request) was not valid.

The error occurred due to an incorrect value in the XML tag in my case. Value should have been =<ns8:exampleTag>123</ns8:exampleTag>= instead of =<ns8:exampleTag>456</ns8:exampleTag>=
